{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6flHEuK9226",
    "outputId": "9f6f85b7-4158-4109-b541-6e5602106057"
   },
   "outputs": [],
   "source": [
    "!pip install stable-baselines3 gym numpy\n",
    "!pip install 'shimmy>=0.2.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_NTM0ggeOWQ"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbCmFprx-UYZ",
    "outputId": "24e3b539-1dd3-4a1a-d612-501afb7bb97c"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "from gazebo_msgs.msg import ModelStates, ModelState\n",
    "from gazebo_msgs.srv import SetModelState\n",
    "import rospy\n",
    "import sys\n",
    "from clover.srv import SetVelocity, SetVelocityRequest, SetRates, SetRatesRequest, Navigate, NavigateRequest\n",
    "import numpy as np\n",
    "from geometry_msgs.msg import Wrench, Vector3, PoseStamped\n",
    "from mavros_msgs.srv import CommandBool, CommandTOL, SetMode\n",
    "from std_msgs.msg import String\n",
    "from mavros_msgs.msg import State\n",
    "from std_srvs.srv import Empty\n",
    "import tf.transformations as transformations\n",
    "import math\n",
    "import random \n",
    "\n",
    "class PositionControlEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    def __init__(self):\n",
    "        super(PositionControlEnv, self).__init__()\n",
    "\n",
    "        # Define the low and high bounds for each action Kp, Kd, Ki\n",
    "        low_bounds = np.array([0, 0, 0], dtype=np.float32)\n",
    "        high_bounds = np.array([1, 1, 0.0001], dtype=np.float32)\n",
    "        \n",
    "        # Create the action space with custom bounds for each action\n",
    "        self.action_space = spaces.Box(low=low_bounds, high=high_bounds, dtype=np.float32)\n",
    "        \n",
    "\n",
    "        # State: current position [-10, 10] + target position\n",
    "        self.observation_space = spaces.Box(low=-1000, high=1000, shape=(3,), dtype=np.float32)\n",
    "\n",
    "        self.max_step_per_episode = 512 \n",
    "        self.flipped = False\n",
    "        rospy.init_node('jupyter_velocity_controller', anonymous=True)\n",
    "        # Create a ROS publisher on the vision_pose topic\n",
    "        self.vision_pose_pub = rospy.Publisher('/mavros/vision_pose/pose', PoseStamped, queue_size=10)\n",
    "        #self.position_subscriber = rospy.Subscriber('/mavros/local_position/pose', PoseStamped, self.position_callback)\n",
    "        self.set_velocity_service = rospy.ServiceProxy('/set_velocity', SetVelocity)\n",
    "        self.set_rates_service = rospy.ServiceProxy('/set_rates', SetRates)\n",
    "        self.navigate_service = rospy.ServiceProxy('/navigate', Navigate)\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        self.generate_random_positions()\n",
    "        self.previous_error = np.array([0.0, 0.0, 0.0])\n",
    "        self.sum_error = np.array([0.0, 0.0, 0.0])\n",
    "        self.current_step = 0\n",
    "        self.step_to_success = 0\n",
    "        self.halfway = False\n",
    "        self.set_velocity(0, 0, 0)\n",
    "        rospy.sleep(3)\n",
    "        self.set_rates(0, 0, 0, 0)\n",
    "        \n",
    "        if not self.flipped:\n",
    "            self.land()\n",
    "        else:\n",
    "            self.flipped = False\n",
    "        #self.reset_world()\n",
    "        '''self.flipped = False\n",
    "        self.set_rates(0, 0, 0, 0.55)\n",
    "        rospy.sleep(2)'''\n",
    "        state = np.array([np.linalg.norm(self.position_error), np.linalg.norm(self.position_error-self.previous_error), np.linalg.norm(self.sum_error+self.position_error)])\n",
    "        return state\n",
    "\n",
    "    def generate_random_positions(self):\n",
    "        # Generate random coordinates\n",
    "        x = random.uniform(-1, 9)\n",
    "        y = random.uniform(-1, 9)\n",
    "        z = random.uniform(0.3, 3)\n",
    "        a = random.uniform(-1, 9)\n",
    "        b = random.uniform(-1, 9)\n",
    "        c = 0.1\n",
    "        self.target_position = np.array([5, 5, 1.5])\n",
    "        self.current_position = np.array([0, 0, 0.1])\n",
    "        #self.target_position = np.array([x, y, z])\n",
    "        #self.current_position = np.array([a, b, c])\n",
    "        self.position_error = self.target_position - self.current_position\n",
    "        self.distance = np.linalg.norm(self.position_error)\n",
    "        print(\"Current start_point: \", self.current_position)\n",
    "        print(\"Current target: \", self.target_position)\n",
    "        \n",
    "\n",
    "    def set_model_state(self):\n",
    "        rospy.wait_for_service('/gazebo/set_model_state')\n",
    "        try:\n",
    "            set_state = rospy.ServiceProxy('/gazebo/set_model_state', SetModelState)\n",
    "\n",
    "            state_msg = ModelState()\n",
    "            state_msg.model_name = 'clover'\n",
    "            state_msg.pose.position.x = self.current_position[0]  # Set your x position\n",
    "            state_msg.pose.position.y = self.current_position[1]  # Set your y position\n",
    "            state_msg.pose.position.z = self.current_position[2]  # Set your z position\n",
    "            # Set orientation using either quaternion or roll, pitch, yaw\n",
    "            state_msg.pose.orientation.x = 0.0\n",
    "            state_msg.pose.orientation.y = 0.0\n",
    "            state_msg.pose.orientation.z = 0.0\n",
    "            state_msg.pose.orientation.w = 1.0\n",
    "            \n",
    "            resp = set_state(state_msg)\n",
    "            print(\"Model state reset.\")\n",
    "        except rospy.ServiceException as e:\n",
    "            print(\"Service set model state call failed: %s\" % e)\n",
    "    \n",
    "    def land(self, land=True):\n",
    "        rospy.wait_for_service('/mavros/cmd/land')\n",
    "        try:\n",
    "            land_service = rospy.ServiceProxy('/mavros/cmd/land', CommandTOL)\n",
    "            land_service(0.0, 0.0, 0.0, 0.0, 0.0)\n",
    "            print(\"Landing starts.\")\n",
    "            while self.check_arm_status() == True:\n",
    "                rospy.sleep(1)\n",
    "            rospy.sleep(5)\n",
    "            print(\"Landing command completed.\")\n",
    "        except rospy.ServiceException as e:\n",
    "            print(\"Service landing call failed: %s\" % e)\n",
    "    \n",
    "\n",
    "    def check_arm_status(self):\n",
    "        try:\n",
    "            current_state = rospy.wait_for_message(\"/mavros/state\", State, timeout=5)\n",
    "            is_armed = current_state.armed\n",
    "            return is_armed\n",
    "            #print(\"Is the drone armed?\", is_armed)\n",
    "        \n",
    "        except rospy.ROSException as e:\n",
    "            print(\"Failed to get state message within timeout:\", str(e))\n",
    "        except rospy.ROSInterruptException:\n",
    "            print(\"Node was shutdown\")\n",
    "        \n",
    "    def reset_world(self):\n",
    "        try:\n",
    "            reset_world_service = rospy.ServiceProxy('/gazebo/reset_world', Empty)\n",
    "            response = reset_world_service()\n",
    "            rospy.sleep(2)\n",
    "            print(\"World reset successful.\", response)\n",
    "        except rospy.ServiceException as e:\n",
    "            print(\"World reset failed: %s\" % e)\n",
    "            \n",
    "    def step(self, action):\n",
    "        if self.current_step ==0:\n",
    "            #self.set_rates(0, 0, 0, 0.579)\n",
    "            #rospy.sleep(5)\n",
    "            self.set_model_state()\n",
    "            #self.current_position, _ = self.get_current_position()\n",
    "            \n",
    "        Kpid = action.reshape(-1)\n",
    "        position_error = self.position_error\n",
    "        #PID control logic \n",
    "        velocity = position_error * Kpid[0] + (position_error-self.previous_error)* Kpid[1] + (position_error + self.sum_error) * Kpid[2]\n",
    "        normalized_v = np.array([x / (abs(x) + 1) for x in velocity]) # velocity ~ [-1,1]\n",
    "        \n",
    "        #print(Kpid)\n",
    "        \n",
    "        self.set_velocity(normalized_v[0], normalized_v[1],normalized_v[2])\n",
    "        rospy.sleep(0.04)# need to be mverify: control frequency 250Hz?\n",
    "        \n",
    "        self.current_step += 1\n",
    "        \n",
    "        \n",
    "        self.current_position, self.flipped = self.get_current_position()\n",
    "        self.previous_error = position_error\n",
    "        self.sum_error += position_error\n",
    "        self.position_error = self.target_position - self.current_position # update position error after taking a step\n",
    "        \n",
    "        if self.flipped: #self.fail_threshold:\n",
    "            self.set_rates(0, 0, 0, 0) #stop flipping\n",
    "            reward = -1000 #self.fail_reward\n",
    "            self.step_to_success = 0\n",
    "            print(\"Drone tends to flip upside down, manual restart\")\n",
    "            done = True\n",
    "            \n",
    "        if not (-2 <= self.current_position[0] < 10) or not (-2 <= self.current_position[1] < 10) or self.current_position[2] > 4:\n",
    "            reward = 0 #self.fail_reward\n",
    "            self.step_to_success = 0\n",
    "            print(\"Drone flies too far, manual restart\")\n",
    "            done = True\n",
    "        elif np.linalg.norm(self.position_error) <= 0.25: #self.success_threshold:\n",
    "            self.step_to_success += 1\n",
    "            done = False\n",
    "            reward = 0\n",
    "            if self.step_to_success == 50:\n",
    "                #average_speed  = self.distance / (self.current_step*0.04)\n",
    "                average_speed  = self.distance / ((self.current_step-50)*0.04) \n",
    "                reward = math.exp(average_speed*10)\n",
    "                print(\"Reach the goal! Average flying speed:\", average_speed, \"m/s\")\n",
    "                #done = True\n",
    "            \n",
    "        elif np.linalg.norm(self.position_error) < (self.distance / 2): \n",
    "            reward = 0\n",
    "            if self.halfway == False:\n",
    "                halfway_speed  = self.distance / (self.current_step*0.04)\n",
    "                reward = math.exp(halfway_speed*5)\n",
    "                self.halfway = True\n",
    "                print(\"Half way done! Keep going\")\n",
    "            self.step_to_success = 0\n",
    "            done = False\n",
    "        else:\n",
    "            self.step_to_success = 0\n",
    "            reward = 0\n",
    "            done = False\n",
    "\n",
    "        if self.current_step >= self.max_step_per_episode: #flexible self.max_steps_per_episode:\n",
    "            print(\"Episode ends\")\n",
    "            reward = 0\n",
    "            done = True\n",
    "        \n",
    "        state = np.array([np.linalg.norm(self.position_error), np.linalg.norm(self.position_error-self.previous_error), np.linalg.norm(self.sum_error+self.position_error)])\n",
    "        \n",
    "        return state, reward, done, {}\n",
    "\n",
    "    \n",
    "    def close(self):\n",
    "        if hasattr(self, 'figure'):\n",
    "            plt.close(self.figure)\n",
    "\n",
    "    #velocity controller\n",
    "    def position_callback(self, msg):\n",
    "        self.current_position = msg.pose\n",
    "        #print(\"Position received:\", self.current_position)\n",
    "        \n",
    "    def get_current_position(self):\n",
    "        try:\n",
    "            model_states = rospy.wait_for_message('/gazebo/model_states', ModelStates, timeout=5)\n",
    "            PI_2 = math.pi / 6 #modified to prevent large angle\n",
    "            #position_message = rospy.wait_for_message('/mavros/local_position/pose', PoseStamped, timeout=5)\n",
    "            drone_index = model_states.name.index('clover')  \n",
    "            position = model_states.pose[drone_index].position\n",
    "            #print(\"Current position:\", position.x, position.y, position.z)\n",
    "            \n",
    "            #check if the drone is flipped upside down\n",
    "            orientation = model_states.pose[drone_index].orientation\n",
    "            \n",
    "            quaternion = [orientation.x, orientation.y, orientation.z, orientation.w]\n",
    "            #angle = self.quaternion_to_inclined_angle(quaternion)\n",
    "\n",
    "            roll, pitch, yaw = transformations.euler_from_quaternion(quaternion)\n",
    "            flipped = abs(roll) > PI_2 or abs(pitch) > PI_2\n",
    "\n",
    "            return  np.array([position.x, position.y, position.z]), flipped#, angle\n",
    "        except rospy.ROSException as e:\n",
    "            print(\"Failed to get position message within timeout:\", str(e))\n",
    "            return np.array([0, 0, 0]), True#, 0\n",
    "        except rospy.ROSInterruptException:\n",
    "            print(\"Node was shutdown\")\n",
    "            \n",
    "    def quaternion_to_inclined_angle(self, q):\n",
    "            # Normalize the quaternion\n",
    "            q = q / np.linalg.norm(q)\n",
    "            qw, qx, qy, qz = q\n",
    "            \n",
    "            # Transform the local up vector (0, 0, 1) using the quaternion\n",
    "            # This effectively gives us the drone's up vector in global coordinates\n",
    "            up_vector_transformed = np.array([\n",
    "                2*(qx*qz + qw*qy),\n",
    "                2*(qy*qz - qw*qx),\n",
    "                2*(qw*qw - 0.5 + qz*qz)\n",
    "            ])\n",
    "            \n",
    "            # Calculate the angle between the transformed up vector and the global up vector (0, 0, 1)\n",
    "            # This is the inclined angle of the drone relative to the vertical axis\n",
    "            dot_product = up_vector_transformed.dot(np.array([0, 0, 1]))\n",
    "            inclined_angle = np.arccos(dot_product)\n",
    "            \n",
    "            # Convert the angle to degrees for easier interpretation\n",
    "            inclined_angle_degrees = np.degrees(inclined_angle)\n",
    "            \n",
    "            return inclined_angle_degrees\n",
    "        \n",
    "    def set_velocity(self, vx, vy, vz):\n",
    "        velocity_request = SetVelocityRequest()\n",
    "        velocity_request.vx = vx\n",
    "        velocity_request.vy = vy\n",
    "        velocity_request.vz = vz\n",
    "        velocity_request.frame_id = 'map'\n",
    "        velocity_request.auto_arm = 1\n",
    "        try:\n",
    "            response = self.set_velocity_service(velocity_request)\n",
    "            #print(\"velocity setpoint: \", vx, vy, vz)\n",
    "            \n",
    "        except rospy.ServiceException as e:\n",
    "            print(\"Service call failed: %s\" % e)\n",
    "\n",
    "    def set_rates(self, r, p, y, t):\n",
    "        rates_request = SetRatesRequest()\n",
    "        rates_request.roll_rate = r\n",
    "        rates_request.pitch_rate = p\n",
    "        rates_request.yaw_rate = y\n",
    "        rates_request.thrust = t\n",
    "        rates_request.auto_arm = 1\n",
    "        try:\n",
    "            response = self.set_rates_service(rates_request)\n",
    "            print(\"rates setpoint: \", r, p, y, t)\n",
    "            #print(\"Service call successful. Response: \", response)\n",
    "        except rospy.ServiceException as e:\n",
    "            print(\"Service call failed: %s\" % e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import rospy\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from std_srvs.srv import Empty, EmptyRequest\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback\n",
    "\n",
    "class PauseSimulationCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(PauseSimulationCallback, self).__init__(verbose)\n",
    "        self.pause_service = rospy.ServiceProxy('/gazebo/pause_physics', Empty)\n",
    "        self.unpause_service = rospy.ServiceProxy('/gazebo/unpause_physics', Empty)\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        return True  # return False if you want to stop the training\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        # This method will be called at the start of each rollout\n",
    "        self.unpause_simulation()\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        # This method will be called at the end of each rollout,\n",
    "        # i.e., after collecting a bunch of experience but before updating the policy.\n",
    "        self.pause_simulation()\n",
    "\n",
    "    def pause_simulation(self):\n",
    "        try:\n",
    "            self.pause_service(EmptyRequest())\n",
    "            print(\"Simulation paused\")\n",
    "        except rospy.ServiceException as e:\n",
    "            print(\"Failed to pause simulation: %s\" % e)\n",
    "\n",
    "    def unpause_simulation(self):\n",
    "        try:\n",
    "            self.unpause_service(EmptyRequest())\n",
    "            print(\"Simulation unpaused\")\n",
    "        except rospy.ServiceException as e:\n",
    "            print(\"Failed to unpause simulation: %s\" % e)\n",
    "\n",
    "# Usage\n",
    "env = DummyVecEnv([lambda: PositionControlEnv()])\n",
    "model = PPO(\"MlpPolicy\", env, n_steps=512, verbose=1)\n",
    "\n",
    "# Initialize the callback\n",
    "pause_simulation_callback = PauseSimulationCallback()\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1024, save_path='./models/', name_prefix='PID_DRL_model')\n",
    "# Combine the callbacks\n",
    "callback_list = [pause_simulation_callback, checkpoint_callback]\n",
    "\n",
    "# Start training\n",
    "#model.learn(total_timesteps=10000, callback=callback_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current start_point:  [1.57521632 1.06912925 0.1       ]\n",
      "Current target:  [-0.19573214  1.051712    2.05706316]\n",
      "rates setpoint:  0 0 0 0\n",
      "Landing starts.\n",
      "Landing command completed.\n",
      "Simulation unpaused\n",
      "Model state reset.\n",
      "Half way done! Keep going\n",
      "Reach the goal! Average flying speed: 0.37492084498224443 m/s\n",
      "Current start_point:  [0.44293743 4.12872124 0.1       ]\n",
      "Current target:  [7.7714107  0.68974439 1.95006782]\n",
      "rates setpoint:  0 0 0 0\n",
      "Landing starts.\n",
      "Landing command completed.\n",
      "Model state reset.\n",
      "Half way done! Keep going\n",
      "Reach the goal! Average flying speed: 0.9065463874377099 m/s\n",
      "Current start_point:  [3.20467973 8.49254782 0.1       ]\n",
      "Current target:  [2.4290567  2.9679765  1.75562099]\n",
      "rates setpoint:  0 0 0 0\n",
      "Landing starts.\n",
      "Landing command completed.\n",
      "Model state reset.\n",
      "Simulation paused\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 3   |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 146 |\n",
      "|    total_timesteps | 512 |\n",
      "----------------------------\n",
      "Simulation unpaused\n",
      "Half way done! Keep going\n",
      "Reach the goal! Average flying speed: 0.6994279466641883 m/s\n",
      "Current start_point:  [-0.87138837  5.97228888  0.1       ]\n",
      "Current target:  [6.60746594 0.58350484 1.87931585]\n",
      "rates setpoint:  0 0 0 0\n",
      "Landing starts.\n",
      "Landing command completed.\n",
      "Model state reset.\n",
      "Half way done! Keep going\n",
      "Simulation paused\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4           |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 240         |\n",
      "|    total_timesteps      | 1024        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 1.87353e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.22       |\n",
      "|    explained_variance   | 3.96e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.07e+06    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.000125   |\n",
      "|    std                  | 0.987       |\n",
      "|    value_loss           | 1.08e+07    |\n",
      "-----------------------------------------\n",
      "Simulation unpaused\n",
      "Reach the goal! Average flying speed: 0.7797504806484175 m/s\n",
      "Current start_point:  [ 6.73879085 -0.53952772  0.1       ]\n",
      "Current target:  [2.85084454 8.80466491 0.5117768 ]\n",
      "rates setpoint:  0 0 0 0\n",
      "Landing starts.\n",
      "Landing command completed.\n",
      "Model state reset.\n",
      "Half way done! Keep going\n",
      "Reach the goal! Average flying speed: 1.0595342233059906 m/s\n",
      "Current start_point:  [-0.45411928  4.18152727  0.1       ]\n",
      "Current target:  [4.13766695 8.3531683  2.52806866]\n",
      "rates setpoint:  0 0 0 0\n",
      "Landing starts.\n",
      "Landing command completed.\n",
      "Model state reset.\n",
      "Half way done! Keep going\n",
      "Simulation paused\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 4             |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 377           |\n",
      "|    total_timesteps      | 1536          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9511208e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -4.22         |\n",
      "|    explained_variance   | 5.42e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.22e+06      |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | 2.31e-06      |\n",
      "|    std                  | 0.987         |\n",
      "|    value_loss           | 3.13e+07      |\n",
      "-------------------------------------------\n",
      "Simulation unpaused\n",
      "Reach the goal! Average flying speed: 0.8541060577981815 m/s\n",
      "Current start_point:  [6.34237265 5.51130264 0.1       ]\n",
      "Current target:  [4.2196805  1.84434447 1.94802068]\n",
      "rates setpoint:  0 0 0 0\n",
      "Landing starts.\n",
      "Landing command completed.\n",
      "Model state reset.\n",
      "Half way done! Keep going\n",
      "Reach the goal! Average flying speed: 0.5529316361187429 m/s\n",
      "Current start_point:  [8.07253788 7.71658225 0.1       ]\n",
      "Current target:  [0.88745215 4.41639718 0.37336135]\n",
      "rates setpoint:  0 0 0 0\n",
      "Landing starts.\n",
      "Landing command completed.\n",
      "Model state reset.\n",
      "Simulation paused\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3             |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 547           |\n",
      "|    total_timesteps      | 2048          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4804536e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -4.22         |\n",
      "|    explained_variance   | 8.4e-06       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.38e+07      |\n",
      "|    n_updates            | 250           |\n",
      "|    policy_gradient_loss | -5.88e-05     |\n",
      "|    std                  | 0.987         |\n",
      "|    value_loss           | 4.86e+07      |\n",
      "-------------------------------------------\n",
      "Simulation unpaused\n",
      "Half way done! Keep going\n",
      "Reach the goal! Average flying speed: 0.7218497246133021 m/s\n",
      "Current start_point:  [7.88832287 7.6592775  0.1       ]\n",
      "Current target:  [3.17845218 2.51011598 2.89763897]\n",
      "rates setpoint:  0 0 0 0\n",
      "Landing starts.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/PID_DRL_model_14000_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m, env\u001b[38;5;241m=\u001b[39menv, n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[1;32m    176\u001b[0m     clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 178\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:70\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n\u001b[0;32m---> 70\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_from_buf(), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones), deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/shimmy/openai_gym_compatibility.py:235\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     warn(\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGym v21 environment do not accept options as a reset parameter, options=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m     )\n\u001b[0;32m--> 235\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgym_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "Cell \u001b[0;32mIn[7], line 60\u001b[0m, in \u001b[0;36mPositionControlEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_rates(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflipped:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mland\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 116\u001b[0m, in \u001b[0;36mPositionControlEnv.land\u001b[0;34m(self, land)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLanding starts.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_arm_status() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     \u001b[43mrospy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m rospy\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLanding command completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/ros/noetic/lib/python3/dist-packages/rospy/timer.py:159\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(duration)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m rospy\u001b[38;5;241m.\u001b[39mrostime\u001b[38;5;241m.\u001b[39mget_rostime() \u001b[38;5;241m<\u001b[39m sleep_t \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    156\u001b[0m       rospy\u001b[38;5;241m.\u001b[39mrostime\u001b[38;5;241m.\u001b[39mget_rostime() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m initial_rostime \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    157\u001b[0m           \u001b[38;5;129;01mnot\u001b[39;00m rospy\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mis_shutdown():\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m rostime_cond:\n\u001b[0;32m--> 159\u001b[0m         \u001b[43mrostime_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rospy\u001b[38;5;241m.\u001b[39mrostime\u001b[38;5;241m.\u001b[39mget_rostime() \u001b[38;5;241m<\u001b[39m initial_rostime:\n\u001b[1;32m    162\u001b[0m     time_jump \u001b[38;5;241m=\u001b[39m (initial_rostime \u001b[38;5;241m-\u001b[39m rospy\u001b[38;5;241m.\u001b[39mrostime\u001b[38;5;241m.\u001b[39mget_rostime())\u001b[38;5;241m.\u001b[39mto_sec()\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"./models/PID_DRL_model_14000_steps\", env=env, n_steps=512)\n",
    "model.learn(total_timesteps=10000, callback=callback_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "aj0pxhRW-pSm",
    "outputId": "d94c5d07-c133-42f4-8a51-a037e38ce0db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current start_point:  [0.  0.  0.1]\n",
      "Current target:  [5.  5.  1.5]\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"./models/PID_DRL_model_15000_steps\", env=env, n_steps=512)\n",
    "eval_env = PositionControlEnv()\n",
    "eval_env.max_step_per_episode = 1000\n",
    "obs = eval_env.reset()\n",
    "step_to_success = 0\n",
    "for _ in range(10000): \n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    print(action)\n",
    "    obs, reward, done, info = eval_env.step(action)\n",
    "    if done:\n",
    "        print(\"Game is Over\", info)\n",
    "        break\n",
    "print(\"Game is Over\", info)\n",
    "eval_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
